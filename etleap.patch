diff --git a/internal/provider/pipeline_resource.go b/internal/provider/pipeline_resource.go
index 2c93461..5a8a9ac 100644
--- a/internal/provider/pipeline_resource.go
+++ b/internal/provider/pipeline_resource.go
@@ -5,6 +5,9 @@ package provider
 import (
 	"context"
 	"fmt"
+	"slices"
+	"time"
+
 	speakeasy_boolplanmodifier "github.com/etleap/terraform-provider-etleap/internal/planmodifiers/boolplanmodifier"
 	speakeasy_int64planmodifier "github.com/etleap/terraform-provider-etleap/internal/planmodifiers/int64planmodifier"
 	speakeasy_listplanmodifier "github.com/etleap/terraform-provider-etleap/internal/planmodifiers/listplanmodifier"
@@ -13,6 +16,7 @@ import (
 	speakeasy_stringplanmodifier "github.com/etleap/terraform-provider-etleap/internal/planmodifiers/stringplanmodifier"
 	"github.com/etleap/terraform-provider-etleap/internal/sdk"
 	"github.com/etleap/terraform-provider-etleap/internal/sdk/pkg/models/operations"
+	"github.com/etleap/terraform-provider-etleap/internal/sdk/pkg/models/shared"
 	"github.com/etleap/terraform-provider-etleap/internal/validators"
 	speakeasy_listvalidators "github.com/etleap/terraform-provider-etleap/internal/validators/listvalidators"
 	speakeasy_numbervalidators "github.com/etleap/terraform-provider-etleap/internal/validators/numbervalidators"
@@ -34,6 +38,7 @@ import (
 	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
 	"github.com/hashicorp/terraform-plugin-framework/types"
 	"github.com/hashicorp/terraform-plugin-framework/types/basetypes"
+	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/retry"
 )
 
 // Ensure provider defined types fully satisfy framework interfaces.
@@ -99,25 +104,21 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 				Description: `Specifies whether any remaining export products in the destination created by this pipeline should be deleted. For REDSHIFT and SNOWFLAKE destinations this means tables, and for S3 DATA LAKE destinations this means data output to S3 as well as any tables created in Glue. Defaults to ` + "`" + `false` + "`" + `. Default: false`,
 			},
 			"destination": schema.SingleNestedAttribute{
-				PlanModifiers: []planmodifier.Object{
-					objectplanmodifier.RequiresReplaceIfConfigured(),
-				},
-				Required: true,
+				// [ICE-364] Prevents TF from recreating a resource if certain destination properties are changed
+				// e.g. automatic schema changes
+				PlanModifiers: []planmodifier.Object{},
+				Required:      true,
 				Attributes: map[string]schema.Attribute{
 					"delta_lake": schema.SingleNestedAttribute{
-						PlanModifiers: []planmodifier.Object{
-							objectplanmodifier.RequiresReplaceIfConfigured(),
-						},
-						Optional: true,
+						PlanModifiers: []planmodifier.Object{},
+						Optional:      true,
 						Attributes: map[string]schema.Attribute{
 							"automatic_schema_changes": schema.BoolAttribute{
-								Computed: true,
-								PlanModifiers: []planmodifier.Bool{
-									boolplanmodifier.RequiresReplaceIfConfigured(),
-								},
-								Optional:    true,
-								Default:     booldefault.StaticBool(true),
-								Description: `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
+								Computed:      true,
+								PlanModifiers: []planmodifier.Bool{},
+								Optional:      true,
+								Default:       booldefault.StaticBool(true),
+								Description:   `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
 							},
 							"connection_id": schema.StringAttribute{
 								PlanModifiers: []planmodifier.String{
@@ -167,17 +168,15 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 								Default:     booldefault.StaticBool(false),
 								Description: `If the destination table should retain the history of the source. More information here: https://docs.etleap.com/docs/documentation/56a1503dc499e-update-with-history-retention-mode. Defaults to ` + "`" + `false` + "`" + `. Requires replacement if changed. ; Default: false`,
 							},
+							// [VIK-4496] Add support for schema and table renames
 							"schema": schema.StringAttribute{
-								PlanModifiers: []planmodifier.String{
-									stringplanmodifier.RequiresReplaceIfConfigured(),
-								},
+								PlanModifiers: []planmodifier.String{},
 								Required:    true,
 								Description: `The schema in the destination that the tables will be created in. Requires replacement if changed. `,
 							},
+							// [VIK-4496] Add support for schema and table renames
 							"table": schema.StringAttribute{
-								PlanModifiers: []planmodifier.String{
-									stringplanmodifier.RequiresReplaceIfConfigured(),
-								},
+								PlanModifiers: []planmodifier.String{},
 								Required:    true,
 								Description: `Requires replacement if changed. `,
 							},
@@ -279,19 +278,17 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 						Description: `Requires replacement if changed. `,
 					},
 					"redshift": schema.SingleNestedAttribute{
-						PlanModifiers: []planmodifier.Object{
-							objectplanmodifier.RequiresReplaceIfConfigured(),
-						},
-						Optional: true,
+						// [ICE-364] Prevents TF from recreating a resource if certain destination properties are changed
+						// e.g. automatic schema changes
+						PlanModifiers: []planmodifier.Object{},
+						Optional:      true,
 						Attributes: map[string]schema.Attribute{
 							"automatic_schema_changes": schema.BoolAttribute{
-								Computed: true,
-								PlanModifiers: []planmodifier.Bool{
-									boolplanmodifier.RequiresReplaceIfConfigured(),
-								},
-								Optional:    true,
-								Default:     booldefault.StaticBool(true),
-								Description: `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
+								Computed:      true,
+								PlanModifiers: []planmodifier.Bool{},
+								Optional:      true,
+								Default:       booldefault.StaticBool(true),
+								Description:   `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
 							},
 							"compress_columns": schema.BoolAttribute{
 								Computed: true,
@@ -390,10 +387,9 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 								Default:     booldefault.StaticBool(false),
 								Description: `If the destination table should retain the history of the source. More information here: https://docs.etleap.com/docs/documentation/56a1503dc499e-update-with-history-retention-mode. Defaults to ` + "`" + `false` + "`" + `. Requires replacement if changed. ; Default: false`,
 							},
+							// [VIK-4496] Add support for schema and table renames
 							"schema": schema.StringAttribute{
-								PlanModifiers: []planmodifier.String{
-									stringplanmodifier.RequiresReplaceIfConfigured(),
-								},
+								PlanModifiers: []planmodifier.String{},
 								Optional:    true,
 								Description: `The schema in the destination that the tables will be created in. If this is not specified or set to ` + "`" + `null` + "`" + ` then the schema specified on the connection is used. Requires replacement if changed. `,
 							},
@@ -408,10 +404,9 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 									listvalidator.SizeAtLeast(1),
 								},
 							},
+							// [VIK-4496] Add support for schema and table renames
 							"table": schema.StringAttribute{
-								PlanModifiers: []planmodifier.String{
-									stringplanmodifier.RequiresReplaceIfConfigured(),
-								},
+								PlanModifiers: []planmodifier.String{},
 								Required:    true,
 								Description: `Requires replacement if changed. `,
 							},
@@ -449,19 +444,17 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 						Description: `Requires replacement if changed. `,
 					},
 					"s3_data_lake": schema.SingleNestedAttribute{
-						PlanModifiers: []planmodifier.Object{
-							objectplanmodifier.RequiresReplaceIfConfigured(),
-						},
-						Optional: true,
+						// [ICE-364] Prevents TF from recreating a resource if certain destination properties are changed
+						// e.g. automatic schema changes
+						PlanModifiers: []planmodifier.Object{},
+						Optional:      true,
 						Attributes: map[string]schema.Attribute{
 							"automatic_schema_changes": schema.BoolAttribute{
-								Computed: true,
-								PlanModifiers: []planmodifier.Bool{
-									boolplanmodifier.RequiresReplaceIfConfigured(),
-								},
-								Optional:    true,
-								Default:     booldefault.StaticBool(true),
-								Description: `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
+								Computed:      true,
+								PlanModifiers: []planmodifier.Bool{},
+								Optional:      true,
+								Default:       booldefault.StaticBool(true),
+								Description:   `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
 							},
 							"connection_id": schema.StringAttribute{
 								PlanModifiers: []planmodifier.String{
@@ -537,19 +530,17 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 						Description: `Requires replacement if changed. `,
 					},
 					"snowflake": schema.SingleNestedAttribute{
-						PlanModifiers: []planmodifier.Object{
-							objectplanmodifier.RequiresReplaceIfConfigured(),
-						},
-						Optional: true,
+						// [ICE-364] Prevents TF from recreating a resource if certain destination properties are changed
+						// e.g. automatic schema changes
+						PlanModifiers: []planmodifier.Object{},
+						Optional:      true,
 						Attributes: map[string]schema.Attribute{
 							"automatic_schema_changes": schema.BoolAttribute{
-								Computed: true,
-								PlanModifiers: []planmodifier.Bool{
-									boolplanmodifier.RequiresReplaceIfConfigured(),
-								},
-								Optional:    true,
-								Default:     booldefault.StaticBool(true),
-								Description: `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
+								Computed:      true,
+								PlanModifiers: []planmodifier.Bool{},
+								Optional:      true,
+								Default:       booldefault.StaticBool(true),
+								Description:   `Whether schema changes detected during transformation should be handled automatically or not. Defaults to ` + "`" + `true` + "`" + `. Requires replacement if changed. ; Default: true`,
 							},
 							"clustering_keys": schema.ListAttribute{
 								PlanModifiers: []planmodifier.List{
@@ -594,16 +585,12 @@ func (r *PipelineResource) Schema(ctx context.Context, req resource.SchemaReques
 								Description: `If the destination table should retain the history of the source. More information here: https://docs.etleap.com/docs/documentation/56a1503dc499e-update-with-history-retention-mode. Defaults to ` + "`" + `false` + "`" + `. Requires replacement if changed. ; Default: false`,
 							},
 							"schema": schema.StringAttribute{
-								PlanModifiers: []planmodifier.String{
-									stringplanmodifier.RequiresReplaceIfConfigured(),
-								},
+								PlanModifiers: []planmodifier.String{},
 								Optional:    true,
 								Description: `The schema in the destination that the tables will be created in. If this is not specified or set to ` + "`" + `null` + "`" + ` then the schema specified on the connection is used. Requires replacement if changed. `,
 							},
 							"table": schema.StringAttribute{
-								PlanModifiers: []planmodifier.String{
-									stringplanmodifier.RequiresReplaceIfConfigured(),
-								},
+								PlanModifiers: []planmodifier.String{},
 								Required:    true,
 								Description: `Requires replacement if changed. `,
 							},
@@ -9017,7 +9004,16 @@ func (r *PipelineResource) Create(ctx context.Context, req resource.CreateReques
 	data.RefreshFromSharedPipelineOutput(res1.PipelineOutput)
 	refreshPlan(ctx, plan, &data, resp.Diagnostics)
 
-	// Save updated data into Terraform state
+	// [ICE-364] Etleap monkey-patch; unwrapping the first destination of the response into the TF state.
+	existingDestination := data.Destination.Redshift
+	if existingDestination == nil {
+		data.Destination.Redshift = data.Destinations[0].Destination.Redshift
+		data.Destination.Snowflake = data.Destinations[0].Destination.Snowflake
+		data.Destination.DeltaLake = data.Destinations[0].Destination.DeltaLake
+		data.Destination.S3DataLake = data.Destinations[0].Destination.S3DataLake
+	}
+
+	// [ICE-364] Save updated data into Terraform state
 	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
 }
 
@@ -9065,7 +9061,16 @@ func (r *PipelineResource) Read(ctx context.Context, req resource.ReadRequest, r
 	}
 	data.RefreshFromSharedPipelineOutput(res.PipelineOutput)
 
-	// Save updated data into Terraform state
+	// [ICE-364] Etleap monkey-patch; unwrapping the first destination of the response into the TF state.
+	existingDestination := data.Destination.Redshift
+	if existingDestination == nil {
+		data.Destination.Redshift = data.Destinations[0].Destination.Redshift
+		data.Destination.Snowflake = data.Destinations[0].Destination.Snowflake
+		data.Destination.DeltaLake = data.Destinations[0].Destination.DeltaLake
+		data.Destination.S3DataLake = data.Destinations[0].Destination.S3DataLake
+	}
+
+	// [ICE-364] Save updated data into Terraform state
 	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
 }
 
@@ -9085,6 +9090,39 @@ func (r *PipelineResource) Update(ctx context.Context, req resource.UpdateReques
 
 	id := data.ID.ValueString()
 	pipelineUpdate := *data.ToSharedPipelineUpdate()
+
+	// [ICE-364] Etleap monkey-patch for allowing certain properties to be changed without replacing the pipeline
+	var schemaChanges bool
+	var connectionId string
+	// [VIK-4496] Support for table and schema renames
+	var schema string
+	var table string
+	if data.Destination.Redshift != nil {
+		schemaChanges = data.Destination.Redshift.AutomaticSchemaChanges.ValueBool()
+		connectionId = data.Destination.Redshift.ConnectionID.ValueString()
+		schema = data.Destination.Redshift.Schema.ValueString()
+		table = data.Destination.Redshift.Table.ValueString()
+	} else if data.Destination.Snowflake != nil {
+		schemaChanges = data.Destination.Snowflake.AutomaticSchemaChanges.ValueBool()
+		connectionId = data.Destination.Snowflake.ConnectionID.ValueString()
+		schema = data.Destination.Snowflake.Schema.ValueString()
+		table = data.Destination.Snowflake.Table.ValueString()
+	} else if data.Destination.DeltaLake != nil {
+		schemaChanges = data.Destination.DeltaLake.AutomaticSchemaChanges.ValueBool()
+		connectionId = data.Destination.DeltaLake.ConnectionID.ValueString()
+		schema = data.Destination.DeltaLake.Schema.ValueString()
+		table = data.Destination.DeltaLake.Table.ValueString()
+	}
+
+	var destUpdate *shared.DestinationUpdate = &shared.DestinationUpdate{
+		ConnectionID:           connectionId,
+		AutomaticSchemaChanges: &schemaChanges,
+		Schema:                 &schema,
+		Table:                  &table,
+	}
+	pipelineUpdate.DestinationUpdate = []shared.DestinationUpdate{*destUpdate}
+	// Etleap monkey-patch end
+
 	request := operations.UpdatePipelineRequest{
 		ID:             id,
 		PipelineUpdate: pipelineUpdate,
@@ -9109,6 +9147,31 @@ func (r *PipelineResource) Update(ctx context.Context, req resource.UpdateReques
 		resp.Diagnostics.AddError("unexpected response from API. No response body", debugResponse(res.RawResponse))
 		return
 	}
+
+	// [VIK-4496] Support for table and schema renames
+	// Waiting for rename to complete
+	if destUpdate.Schema != nil || destUpdate.Table != nil {
+		var originalSchema string
+		var originalTable string
+		if data.Destination.Redshift != nil {
+			originalSchema = data.Destination.Redshift.Schema.String()
+			originalTable = data.Destination.Redshift.Table.String()
+		} else if data.Destination.Snowflake != nil {
+			originalSchema = data.Destination.Snowflake.Schema.String()
+			originalTable = data.Destination.Snowflake.Table.String()
+		} else if data.Destination.DeltaLake != nil {
+			originalSchema = data.Destination.DeltaLake.Schema.String()
+			originalTable = data.Destination.DeltaLake.Table.String()
+		}
+
+		_, err = waitPipelineRenamed(ctx, r, id, connectionId, originalSchema, originalTable, *destUpdate.Schema, *destUpdate.Table)
+		if err != nil {
+			resp.Diagnostics.AddError("failed while waiting for pipeline rename", err.Error())
+			return
+		}
+	}
+	// Etleap monkey-patch end
+
 	data.RefreshFromSharedPipelineOutput(res.PipelineOutput)
 	refreshPlan(ctx, plan, &data, resp.Diagnostics)
 	id1 := data.ID.ValueString()
@@ -9138,6 +9201,15 @@ func (r *PipelineResource) Update(ctx context.Context, req resource.UpdateReques
 	data.RefreshFromSharedPipelineOutput(res1.PipelineOutput)
 	refreshPlan(ctx, plan, &data, resp.Diagnostics)
 
+	// [ICE-364] Etleap monkey-patch; unwrapping the first destination of the response into the TF state.
+	existingDestination := data.Destination.Redshift
+	if existingDestination == nil {
+		data.Destination.Redshift = data.Destinations[0].Destination.Redshift
+		data.Destination.Snowflake = data.Destinations[0].Destination.Snowflake
+		data.Destination.DeltaLake = data.Destinations[0].Destination.DeltaLake
+		data.Destination.S3DataLake = data.Destinations[0].Destination.S3DataLake
+	}
+
 	// Save updated data into Terraform state
 	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
 }
@@ -9188,3 +9260,75 @@ func (r *PipelineResource) Delete(ctx context.Context, req resource.DeleteReques
 func (r *PipelineResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {
 	resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root("id"), req.ID)...)
 }
+
+// [VIK-4496] Etleap monkey-patch functions for shchema and table rename
+func waitPipelineRenamed(ctx context.Context, r *PipelineResource, pipelineId string, destinationId string, originalSchema string, originalTable string, newSchema string, newTable string) (*operations.GetPipelineRequest, error) {
+	const (
+		timeout = 15 * time.Minute
+	)
+
+	stateConf := &retry.StateChangeConf{
+		Pending:    []string{originalSchema + "." + originalTable},
+		Target:     []string{newSchema + "." + newTable},
+		Refresh:    getPipelineSchemaAndTable(ctx, r, pipelineId, destinationId),
+		Timeout:    timeout,
+		MinTimeout: 10 * time.Second,
+		Delay:      30 * time.Second,
+	}
+
+	outputRaw, err := stateConf.WaitForStateContext(ctx)
+
+	if output, ok := outputRaw.(*operations.GetPipelineRequest); ok {
+		return output, err
+	}
+
+	return nil, err
+}
+
+func getPipelineSchemaAndTable(ctx context.Context, r *PipelineResource, pipelineId string, destinationId string) retry.StateRefreshFunc {
+	return func() (interface{}, string, error) {
+
+		request := operations.GetPipelineRequest{
+			ID: pipelineId,
+		}
+		res, err := r.client.Pipeline.Get(ctx, request)
+
+		if err != nil {
+			return nil, "", fmt.Errorf("failed to get pipeline %s: %w", pipelineId, err)
+		}
+
+		if res == nil || res.PipelineOutput == nil || res.PipelineOutput.Destinations == nil {
+			return nil, "", fmt.Errorf("missing expected body for pipeline %s, response: %s", pipelineId, debugResponse(res.RawResponse))
+		}
+
+		destinationIndex := slices.IndexFunc(res.PipelineOutput.Destinations, func(destination shared.DestinationInfoAndPipelineVersions) bool {
+			return destination.Destination.DestinationRedshift.ConnectionID == destinationId
+		})
+
+		if destinationIndex == -1 {
+			return nil, "", fmt.Errorf("missing expected destination %s for pipeline %s, response: %s", destinationId, pipelineId, debugResponse(res.RawResponse))
+		}
+
+		var finalSchemaName string
+		var finalTableName string
+		for _, destination := range res.PipelineOutput.Destinations {
+			if destination.Destination.DestinationRedshift != nil && destination.Destination.DestinationRedshift.ConnectionID == destinationId {
+				finalSchemaName = *destination.Destination.DestinationRedshift.Schema
+				finalTableName = destination.Destination.DestinationRedshift.Table
+			} else if destination.Destination.DestinationSnowflake != nil && destination.Destination.DestinationSnowflake.ConnectionID == destinationId {
+				finalSchemaName = *destination.Destination.DestinationSnowflake.Schema
+				finalTableName = destination.Destination.DestinationSnowflake.Table
+			} else if destination.Destination.DestinationDeltaLake != nil && destination.Destination.DestinationDeltaLake.ConnectionID == destinationId {
+				finalSchemaName = destination.Destination.DestinationDeltaLake.Schema
+				finalTableName = destination.Destination.DestinationDeltaLake.Table
+			}
+
+			if finalTableName != "" {
+				// Found the lucky one
+				break
+			}
+		}
+
+		return res, finalSchemaName + "." + finalTableName, nil
+	}
+}
diff --git a/internal/provider/provider.go b/internal/provider/provider.go
index 6a79939..0d80495 100644
--- a/internal/provider/provider.go
+++ b/internal/provider/provider.go
@@ -4,6 +4,9 @@ package provider
 
 import (
 	"context"
+	"crypto/tls"
+	"net/http"
+
 	"github.com/etleap/terraform-provider-etleap/internal/sdk"
 	"github.com/etleap/terraform-provider-etleap/internal/sdk/pkg/models/shared"
 	"github.com/hashicorp/terraform-plugin-framework/datasource"
@@ -23,10 +26,12 @@ type EtleapProvider struct {
 }
 
 // EtleapProviderModel describes the provider data model.
+// [VIK-6259] Adding support for self-signed certificates
 type EtleapProviderModel struct {
-	ServerURL types.String `tfsdk:"server_url"`
-	Username  types.String `tfsdk:"username"`
-	Password  types.String `tfsdk:"password"`
+	ServerURL                 types.String `tfsdk:"server_url"`
+	Username                  types.String `tfsdk:"username"`
+	Password                  types.String `tfsdk:"password"`
+	SkipCertificateValidation types.Bool   `tfsdk:"skip_certificate_validation"`
 }
 
 func (p *EtleapProvider) Metadata(ctx context.Context, req provider.MetadataRequest, resp *provider.MetadataResponse) {
@@ -51,6 +56,10 @@ func (p *EtleapProvider) Schema(ctx context.Context, req provider.SchemaRequest,
 				Optional:  true,
 				Sensitive: true,
 			},
+			// [VIK-6259] Adding support for self-signed certificates
+			"skip_certificate_validation": schema.BoolAttribute{
+				Optional: true,
+			},
 		},
 	}
 }
@@ -77,9 +86,19 @@ func (p *EtleapProvider) Configure(ctx context.Context, req provider.ConfigureRe
 		Password: password,
 	}
 
+	// [VIK-6259] Adding support for self-signed certificates
+	transport := &http.Transport{
+		TLSClientConfig: &tls.Config{
+			InsecureSkipVerify: data.SkipCertificateValidation.ValueBool(),
+		},
+	}
+
 	opts := []sdk.SDKOption{
 		sdk.WithServerURL(ServerURL),
 		sdk.WithSecurity(security),
+		sdk.WithClient(&http.Client{
+			Transport: transport,
+		}),
 	}
 	client := sdk.New(opts...)
 
diff --git a/internal/sdk/pkg/models/shared/destinationdeltalake.go b/internal/sdk/pkg/models/shared/destinationdeltalake.go
index a0e219a..ee5dc16 100644
--- a/internal/sdk/pkg/models/shared/destinationdeltalake.go
+++ b/internal/sdk/pkg/models/shared/destinationdeltalake.go
@@ -66,7 +66,8 @@ func (d DestinationDeltaLake) MarshalJSON() ([]byte, error) {
 }
 
 func (d *DestinationDeltaLake) UnmarshalJSON(data []byte) error {
-	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
+	// [VIK-4496] Etleap monkey-patch: the unmarshal function allows unknown fields for ignoring schemaChangingTo and tableChangingTo
+	if err := utils.UnmarshalJSON(data, &d, "", false, false); err != nil {
 		return err
 	}
 	return nil
diff --git a/internal/sdk/pkg/models/shared/destinationredshift.go b/internal/sdk/pkg/models/shared/destinationredshift.go
index f933d10..593286e 100644
--- a/internal/sdk/pkg/models/shared/destinationredshift.go
+++ b/internal/sdk/pkg/models/shared/destinationredshift.go
@@ -64,7 +64,8 @@ func (d DestinationRedshift) MarshalJSON() ([]byte, error) {
 }
 
 func (d *DestinationRedshift) UnmarshalJSON(data []byte) error {
-	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
+	// [VIK-4496] Etleap monkey-patch: the unmarshal function allows unknown fields for ignoring schemaChangingTo and tableChangingTo
+	if err := utils.UnmarshalJSON(data, &d, "", false, false); err != nil {
 		return err
 	}
 	return nil
diff --git a/internal/sdk/pkg/models/shared/destinationsnowflake.go b/internal/sdk/pkg/models/shared/destinationsnowflake.go
index 72f1444..9f93273 100644
--- a/internal/sdk/pkg/models/shared/destinationsnowflake.go
+++ b/internal/sdk/pkg/models/shared/destinationsnowflake.go
@@ -58,7 +58,8 @@ func (d DestinationSnowflake) MarshalJSON() ([]byte, error) {
 }
 
 func (d *DestinationSnowflake) UnmarshalJSON(data []byte) error {
-	if err := utils.UnmarshalJSON(data, &d, "", false, true); err != nil {
+	// [VIK-4496] Etleap monkey-patch: the unmarshal function allows unknown fields for ignoring schemaChangingTo and tableChangingTo
+	if err := utils.UnmarshalJSON(data, &d, "", false, false); err != nil {
 		return err
 	}
 	return nil
diff --git a/internal/sdk/pkg/models/shared/destinationtypes.go b/internal/sdk/pkg/models/shared/destinationtypes.go
index 1028d6c..5cbd7a3 100644
--- a/internal/sdk/pkg/models/shared/destinationtypes.go
+++ b/internal/sdk/pkg/models/shared/destinationtypes.go
@@ -103,7 +103,8 @@ func (u *DestinationTypes) UnmarshalJSON(data []byte) error {
 	switch dis.Type {
 	case "REDSHIFT":
 		destinationRedshift := new(DestinationRedshift)
-		if err := utils.UnmarshalJSON(data, &destinationRedshift, "", true, true); err != nil {
+		// [VIK-4496] Etleap monkey-patch: the unmarshal function allows unknown fields for ignoring schemaChangingTo and tableChangingTo
+		if err := utils.UnmarshalJSON(data, &destinationRedshift, "", true, false); err != nil {
 			return fmt.Errorf("could not unmarshal expected type: %w", err)
 		}
 
@@ -112,7 +113,8 @@ func (u *DestinationTypes) UnmarshalJSON(data []byte) error {
 		return nil
 	case "SNOWFLAKE":
 		destinationSnowflake := new(DestinationSnowflake)
-		if err := utils.UnmarshalJSON(data, &destinationSnowflake, "", true, true); err != nil {
+		// [VIK-4496] Etleap monkey-patch: the unmarshal function allows unknown fields for ignoring schemaChangingTo and tableChangingTo
+		if err := utils.UnmarshalJSON(data, &destinationSnowflake, "", true, false); err != nil {
 			return fmt.Errorf("could not unmarshal expected type: %w", err)
 		}
 
@@ -121,7 +123,8 @@ func (u *DestinationTypes) UnmarshalJSON(data []byte) error {
 		return nil
 	case "DELTA_LAKE":
 		destinationDeltaLake := new(DestinationDeltaLake)
-		if err := utils.UnmarshalJSON(data, &destinationDeltaLake, "", true, true); err != nil {
+		// [VIK-4496] Etleap monkey-patch: the unmarshal function allows unknown fields for ignoring schemaChangingTo and tableChangingTo
+		if err := utils.UnmarshalJSON(data, &destinationDeltaLake, "", true, false); err != nil {
 			return fmt.Errorf("could not unmarshal expected type: %w", err)
 		}
 
